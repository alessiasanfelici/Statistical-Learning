---
title: "Mandatory Assignment 2 - STK-IN4300"
output:
  html_document: default
  pdf_document: default
  word_document: default
author: "Alessia Sanfelici"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
```

# Problem 1: Regression

The first problem of the assignment consists in the analysis of data coming from a study on data toxicity. The dataset is composed of 546 observations and 9 variables. The aim of the analysis is to predict acute aquatic toxicity, which is measured through a variable called $LC50$ (the concentration that causes death in 50% of the planktonic crustacean $Daphnia magna$ over a test duration of 48 hours). The other 8 variables, used for the prediction, are:

-   $TPSA$: the topological polar surface area calculated by means of a contribution method that takes into account nitrogen, oxygen, potassium and sulphur;

-   $SAacc$: the Van der Waals surface area (VSA) of atoms that are acceptors of hydrogen bonds;

-   $H050$: the number of hydrogen atoms bonded to heteroatoms;

-   $MLOGP$: the lipophilicity of a molecule, this being the driving force of narcosis;

-   $RDCHI$: a topological index that encodes information about molecular size and branching;

-   $GATS1p$: information on molecular polarisability;

-   $nN$: the number of nitrogen atoms present in the molecule;

-   $C040$: the number of carbon atoms of a certain type, including esters, carboxylic acids, thioesters, carbamic acids, nitriles, etc.;

```{r}
df_toxicity <- read.csv("aquatic_toxicity.csv", sep = ";", header = FALSE)
colnames(df_toxicity) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p","nN", "C040", "LC50")
# create the dichotomic version of the dataset (where the count variables are transformed using 0/1 dummy encoding)

df_toxicity_dummy <- df_toxicity
df_toxicity_dummy["H050"] <- c(as.integer(df_toxicity_dummy ["H050"]>0))
df_toxicity_dummy ["nN"] <- c(as.integer(df_toxicity_dummy ["nN"]>0))
df_toxicity_dummy ["C040"] <- c(as.integer(df_toxicity_dummy ["C040"]>0))

```

## Linear Regression

Firstly, I want to compare the results of the application of linear regression with both the original dataset and a modified dataset, where all the count variables have been transformed using a 0/1 dummy encoding where 0 represents absence of the specific atom and 1 represents presence of the specific atoms. For this reason, I have already created (above) a dicothomic version of the dataset.

Now, I can divide both the datasets into train and test sets, containing 70% and 30% of data, respectively, and then fit linear regression in the two cases and compare the obtained results.

```{r}
set.seed(1)

# create train and test data
sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ]
test_dummy <- df_toxicity_dummy[!sample, ]
```

```{r}
# linear regression model with original dataset
linear_model <- lm(LC50 ~ . , data = train)
summary(linear_model)
```

```{r}
prediction <- predict.lm(linear_model, test)
train_error <- mean(linear_model$residuals^2)
test_error <- mean((test$LC50 - prediction)^2)
sprintf("Train error: %s", train_error) 
sprintf("Test error: %s", test_error) 
```

```{r}
# linear regression model with dummy dataset
linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
summary(linear_model_dummy)
```

```{r}
prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
train_error_dummy <- mean(linear_model_dummy$residuals^2)
test_error_dummy <- mean((test_dummy$LC50 - prediction_dummy)^2)
sprintf("Train error dummy: %s", train_error_dummy) 
sprintf("Test error dummy: %s", test_error_dummy)
```

In the two cases, the coefficients have different meanings. If for dummy variables they represent the effect that the presence of the atoms has on the target variable, when dealing with linear values, they represent the change in the dependent variable associated with a one-unit change in that continuous predictor while holding all other predictors constant.

By comparing the summary of the two models, we can notice that the significance of the coefficients is similar. The two main differences can be seen for $nN$ and $RDCHI$ coefficients. The former is significant for the first model but not for the second one. On the contrary, the latter has a smaller p-value in the first model than in the second one.

Focusing on the errors, we can easily notice that the train and the test errors associated to the original dataset are smaller than the ones related to the dichotomic dataset.

I then repeat the same procedure 200 times, such that each time I create a new training/test split and I apply linear regression to both the versions of the dataset. The idea behind the repetition is the following: it allows to assess the stability and robustness of the models across different data samples. This repeated process helps in accounting for the potential variability in model performance due to the randomness in the split of data. In addition, it also provides more reliable estimates of the average performance of the model.

```{r}
# repeat the process 200 times 
repetitions <- 200
train_error <- 0
test_error <- 0
test_errors_lr <- numeric(repetitions)
train_error_dummy <- 0
test_error_dummy <- 0
test_errors_dummy_lr <- numeric(repetitions)

for (i in 1:repetitions){
  sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
  train <- df_toxicity[sample, ]
  test <- df_toxicity[!sample, ]
  train_dummy <- df_toxicity_dummy[sample, ]
  test_dummy <- df_toxicity_dummy[!sample, ]

  linear_model <- lm(LC50 ~ . , data = train)
  prediction <- predict.lm(linear_model, test)
  train_error <- train_error + mean(linear_model$residuals^2)/repetitions
  test_error <- test_error + mean((test$LC50 - prediction)^2)/repetitions
  test_errors_lr[i-1] <- mean((test$LC50 - prediction)^2)/repetitions

  linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
  prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
  train_error_dummy <- train_error_dummy + mean(linear_model_dummy$residuals^2)/repetitions
  test_error_dummy <- test_error_dummy + mean((test_dummy$LC50 -
                                                 prediction_dummy)^2)/repetitions
  test_errors_dummy_lr[i-1] <- mean((test_dummy$LC50 - prediction_dummy)^2)/repetitions

}

```

```{r}
boxplot(list(test_errors_lr, test_errors_dummy_lr), col = c("red", "blue"), xlab = "Dataset", ylab = "Test Errors")

legend("bottomleft", legend = c("Original dataset", "Dichotomic dataset"), fill = c("red", "blue"))
```

```{r}
sprintf("Train error: %s", train_error) 
sprintf("Test error: %s", test_error) 
sprintf("Train error dummy: %s", train_error_dummy) 
sprintf("Test error dummy: %s", test_error_dummy) 
```

As it is possible to see from both the plot and the results of the errors, in general, the use of the dataset with dicothomic count variables produces worse results. This is due to the fact that using dummy variables is an oversimplification and thus valuable information is ignored.

Due to this result, in the next steps I'm going to focus only on the original dataset, without taking into account the dichotomic dataset.

## Variable selection

The next step consists in comparing different variable selection procedures with different stopping criteria. In particular, I'm focusing on Backward Elimination and Forward Selection, both tested with AIC and BIC.

```{r}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]

full.model <- lm(LC50 ~ . , data = train)
null.model <- lm(LC50 ~ 1, data = train)
```

```{r}
# Backward elimination with BIC stopping criterion
backward_bic <- step(full.model, direction = "backward", k = log(nrow(train)), trace = FALSE)
print("Backward elimination with BIC stopping criterion")
backward_bic
```

```{r}
# Backward elimination with AIC stopping criterion
backward_aic <- step(full.model, direction = "backward", k = 2, trace = FALSE)
print("Backward elimination with AIC stopping criterion")
backward_aic
```

```{r}
# Forward selection with BIC stopping criterion
forward_bic <- step(null.model, direction = "forward", scope = formula(full.model), k = log(nrow(train)), trace = FALSE)
print("Forward selection with BIC stopping criterion")
forward_bic
```

```{r}
# Forward selection with AIC stopping criterion
forward_aic <- step(null.model, direction = "forward", scope = formula(full.model), k = 2, trace = FALSE)
print("Forward selection with AIC stopping criterion")
forward_aic
```

As shown from the results of the 4 methods, the resulting models are the same in all the cases. All of them contain the same 6 variables (the coefficients are different, but the variables are the same): TPSA, SAacc, ML0GP, RDCHI, GATS1p and nN. In particular, focusing on the same variable selection technique, the results obtained with AIC and BIC are the same.

## Ridge Regression

Now, I study the performance of ridge regression, using both 5-fold cross-validation and a bootstrap procedure (considering 100 bootstrap iterations) . The idea is to find the optimal complexity parameter among a grid of lambda values.

```{r}
library(glmnet)
x <- as.matrix(subset(train, select = -LC50))
y <- as.vector(train$LC50)
# Create a sequence of lambda values to try
lambda_seq <- 10^seq(-5, 5, length = 100)

# 5-fold cross-validation
ridge_cv_model <- cv.glmnet(x, y, alpha = 0, lambda = lambda_seq, nfolds = 5)

print(ridge_cv_model)

```

```{r}
# Plot the MSE with respect to lambda
plot(ridge_cv_model, ylab = "MSE", xlim = c(-5, 5))
```

```{r}
best_lambda <- ridge_cv_model$lambda[which.min(ridge_cv_model$cvm)]
sprintf("The best value of lambda is %s", best_lambda)
```

So, I compute the train and test error for the model with the optimal value of lambda:

```{r}
ridge_best <- glmnet(x, y, alpha = 0, lambda = best_lambda)
ridge_cv_train_err <- mean((predict(ridge_best, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_cv_test_err <- mean((predict(ridge_best, as.matrix(subset(test, select = -LC50))) -
                            as.vector(test$LC50))^2)
sprintf("Ridge cv train error: %s", ridge_cv_train_err)
sprintf("Ridge cv test error: %s", ridge_cv_test_err)
```

```{r}
library(boot)
library(pracma)

# Bootstrap
ridge_fit <- function(data, indices, lambda_seq) {
  data_train <- data[indices, ]
  X_train <- as.matrix(subset(data_train, select = -LC50))
  y_train <- as.vector(data_train$LC50)
  fit <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_seq)
  y_pred_train <- predict(fit, newx = X_train, s = lambda_seq)
  mse <- sum((y_train - y_pred_train)^2) / length(y_train)
  return (mse) 
}

mse_boot <- c()
for (i in 1:100){
  lambda = lambda_seq[i]
  boot_results <- boot(data = train, statistic = ridge_fit, R = 100, lambda = lambda)
  mse_boot[i] = mean(boot_results$t)
}
```

```{r}
plot(log10(ridge_cv_model$lambda), ridge_cv_model$cvm, ylab = "MSE", type = "l", xlab = "log(lambda)", col = "red", ylim = c(1.2, 3))
lines(log10(lambda_seq), mse_boot, col = "black", type = "l", ylim = c(1, 3))
legend("topleft", legend = c("5-fold Cross-Validation", "Bootstrap"), col = c("red", "black"), lty = 1)

```

```{r}
best_lambda_boot <- lambda_seq[which.min(mse_boot)]
sprintf("The best value of lambda is %s", best_lambda_boot)
```

So, I compute the train and test error for the model with the optimal value of lambda:

```{r}
ridge_best_boot <- glmnet(x, y, alpha = 0, lambda = best_lambda_boot)
ridge_boot_train_err <- mean((predict(ridge_best_boot, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_boot_test_err <- mean((predict(ridge_best_boot, as.matrix(subset(test, 
                                    select = -LC50))) - as.vector(test$LC50))^2)
sprintf("Ridge bootstrap train error: %s", ridge_boot_train_err)
sprintf("Ridge bootstrap test error: %s", ridge_boot_test_err)

```

The train and test errors resulting form the two methods are very similar to each other, especially the test error. This means that the two methods work similarly, as it is also shown in the figure above, which allows to compare the MSE w.r.t lambda in the two cases.

## Generalized Additive Model

Next, I consider possible non linear effects, fitting a generalized additive model in which the effects of the variables are fitted using smoothing splines. I decided to compare different levels of complexity for the smoothing splines. In particular, I focused on degrees of freedom from 3 to 10.

```{r}
library(gam)

degrees = 3:10

train_error_gam <- zeros(length(degrees), 1)
test_error_gam <- zeros(length(degrees), 1)

# Fit GAM and compute the errors for each degree of freedom
for (k in degrees){
  gam_model <- gam(LC50 ~ s(TPSA, df = k) + s(SAacc, df = k) + s(H050, df = k) + 
                     s(MLOGP, df = k) + s(RDCHI, df = k) + s(GATS1p, df = k) + 
                     s(nN, df = k) + s(C040, df = k), data = train)
  
  train_error_gam[k-2] <- mean((predict(gam_model, subset(train, select = -LC50)) -
                                 train$LC50)^2)
  test_error_gam[k-2] <- mean((predict(gam_model, subset(test, select = -LC50)) -
                                 test$LC50)^2)
}


```

```{r}
# Create a dataframe containing the errors for each degree of freedom
df_gam <- data.frame(degrees, train_error_gam, test_error_gam)
names(df_gam) <- c("Degrees of Freedom", "Train Error", "Test Error")
df_gam

```

I can immediately notice, from the above table, that the lowest test errors are related to small degrees of freedom. In particular, for the final comparison, I will use the result related to degree 3, which correspond to the lowest value of test error.

## Regression Tree

The next step is to fit a model using a regression tree. The procedure is the following:

```{r}
library(rpart)

set.seed(1)

# Grow the full tree
full_tree <- rpart(LC50 ~ . , data = train, control = rpart.control(cp=0))
plotcp(full_tree)

```

```{r}
full_tree$cptable

```

The lowest error is given y a cp = 0.0116540354. So, I prune the tree according to this value. This is the obtained tree:

```{r}
# Prune the tree
pruned_tree <- prune(full_tree, cp = 0.0116540354)
par(mar = c(1, 1, 1, 1))
plot(pruned_tree, main = "Pruned Tree")
text(pruned_tree, cex = 0.5, col = "blue")

```

Then, I compute the train and the test error for the pruned tree:

```{r}
# Train and test error for the pruned tree

train_error_tree <- mean((predict(pruned_tree, subset(train, select = -LC50)) -
                           train$LC50)^2)
test_error_tree <- mean((predict(pruned_tree, subset(test, select = -LC50)) - 
                          test$LC50)^2)
sprintf("Train error Regression Tree: %s", train_error_tree)
sprintf("Test error Regression Tree: %s", test_error_tree)
```

## Comparison of all the methods

Now that I have applied all the methods, I can make a comparison between all of them, in order to draw some conclusions.

```{r}
methods <- c("Linear Regression", "Dummy Linear Regression", "Variable selection",
            "Ridge Cross-Validation", "Ridge Bootstrap", "GAM", "Regression Tree")
train_errors <- c(train_error, train_error_dummy, mean(backward_bic$residuals^2),
                 ridge_cv_train_err, ridge_boot_train_err, train_error_gam[1],
                 train_error_tree)
test_errors <- c(test_error, test_error_dummy, mean((test$LC50 - 
                                                     predict.lm(backward_bic, test))^2),
               ridge_cv_test_err, ridge_boot_test_err, test_error_gam[1],
               test_error_tree)
# Dataframe containing all the errors related to each method
df_comparison <- data.frame(methods, train_errors, test_errors)
names(df_comparison) <- c("Method", "Train Error", "Test Error")
df_comparison

```

The above table shows the train and the test errors for all the different methods used by now. If we consider the best method as the one that minimizes the test error, then in this case the best one is Linear Regression. All the other methods give a larger test error. 

We can also see that GAM and Regression Tree have a lower train error but a larger test error with respect to the other methods. This means that these techniques work very well with the train sample, but when dealing with new data (the test sample) their performance is worse, thus the test error is larger. This behavior suggest a problem of overfitting for these two methodologies.



# Problem 2: Classification

The second problem of this work is based on the study of the Pima Indians Diabetes Database, containing information about 768 women from a population (Pima indians) that is particularly susceptible to diabetes. The response variable $diabetes$ identifies two classes ("pos" and "neg"), depending whether a person involved in the study has developed the disease or not. The dataset contains also other 8 numeric variables:

-   $pregnant$: number of pregnancies;

-   $glucose$: plasma glucose concentration at 2 h in an oral glucose tolerance test;

-   $pressure$: diastolic blood pressure (mm Hg);

-   $triceps$: triceps skin fold thickness (mm);

-   $insulin$: 2-h serum insulin (ÂµU/mL);

-   $mass$: body mass index (kg/m2);

-   $pedigree$: diabetes pedigree function;

-   $age$: age (years);

```{r}
library(mlbench)

data(PimaIndiansDiabetes)
diabetes_df <- as.data.frame(PimaIndiansDiabetes)
diabetes_df["diabetes"] <- c(as.integer(diabetes_df["diabetes"] == "pos"))
head(diabetes_df)

```

I randomly split the dataset into a training set (70% of the observations) and a test set (remaining 30% of the observations), such that the class distributions (i.e. the empirical distribution of diabetes) is similar in the two sets.

```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- diabetes_df[diabetes_df["diabetes"] == "1", ]
negatives_df <- diabetes_df[diabetes_df["diabetes"] == "0", ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- train_data[sample(1:nrow(train)), ]
test_data <- test_data[sample(1:nrow(test)), ]
```

## kNN

The first step for this classification problem is to apply a k-Nearest Neighbor method. The idea is to compare the error estimates obtained using 5-fold cross-validation, Leave-One-Out cross-validation and the actual test error I would have obtained when fitting k-NN for the different values of k. The following plot shows the obtained results.

```{r, include=FALSE}
library(class)
library(cvms)
library(kknn)
library(Rfast)
```

```{r}
fold5_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))

plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.25, 0.45), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = 
         c("blue", "red", "green"), lty = 1)

```

It can be immediately noticed that cross-validation (both with 5-folds and LOO) tends to underestimate the error, especially for small values of k.

I save the values of the lowest test errors related to each of the three methods.

```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)

```

```{r}
test_err_5fold <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo <- 1- loo_errors$crit[k_min_loo]
test_err_knn <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold)
sprintf('Test error with LOO cv: %s', test_err_loo)
sprintf('Actual test error with kNN: %s', test_err_knn)

```

## Generalized Additive Model

Next, I fit a Generalized Additive Model with splines and use a variable selection method to find the best model.

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```

The result shows that only 4 variables are significant, so I have to remove the non significant variables from the model:

```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ s(glucose, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```

Now, all the variables are significant.

The following plots show the relationship between the 4 significant variables and the response.

```{r}
par(mfrow = c(2,2))
plot(model_gam)

```

Finally, I compute the train error and the test error for GAM:

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error)
```

## Classification tree

Now, I start to consider tree-based methods for classifications. The first model I fit is the classification tree, starting from a full tree and then pruning it.

```{r}
set.seed(1)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp=0), 
                             method = "class")
plotcp(classification_tree)

```

```{r}

classification_tree$cptable
```

The minimum value of cp = 0.010582011, so I prune it at this value.

```{r}
class_tree <- prune(classification_tree, cp = 0.010582011)
par(mar = c(2, 2, 2, 2))
plot(class_tree, main = "Pruned Tree")
text(class_tree, cex = 0.5, col = "blue")

```

The corresponding train and test errors are:

```{r}
tree_train_error <- mean(predict(class_tree, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error)

tree_test_error <- mean(predict(class_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error)

```

## Bagged trees

I now consider another tree-based method, by constructing and evaluating an ensemble of bagged trees. The train and the test errors are then computed.

```{r}
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error)

bagged_test_error <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Test error Bagged Trees: %s", bagged_test_error)

```

## Random forest

The last tree-based method is a Random Forest, for which I used 100 trees. The train and the test errors are then computed.

```{r}
library(randomForest)

random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100)
forest_train_error <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                           != train_data$diabetes)
sprintf("Train error Random Forest: %s", forest_train_error)

forest_test_error <- mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
                        != test_data$diabetes)
sprintf("Test error Random Forest: %s", forest_test_error)

```

Comparing the results of the errors of the three tree-based methods, it is clear that the results are similar. The test errors of the first two techniques are very close, while for Random Forest the test error is a bit higher.

## Comparison of all the methods

After the fitting of all these classification methods, it is reasonable to make a comparison of all of them, in order to understand which performs better. Since I consider the best method as the one minimizing the test error, I just need the test error values to evaluate the performance of each technique.

```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
df_comparison <- data.frame(methods, test_errors)
names(df_comparison) <- c("Method", "Test Error")
df_comparison
```

The above table shows that the lowest test error is obtained with a kNN fitted using 5-fold cross-validation. The result obtained with LOO cross-validation is very good as well. In general, all the test error values shown in the table are similar, so the different models have a similar performance.

## New Dataset

Looking more closely at the data, it has been noted that several values are impossible (e.g., a body mass index of 0). This is due to the fact that some of the observations are actually not zeros, but missing values. So, I now consider the correct dataset and compare the old results of all the methods implemented in the previous points to those obtained after removing all observations that contain missing values.

```{r}
data(PimaIndiansDiabetes2)
new_diabetes_df <- as.data.frame(PimaIndiansDiabetes2)
new_diabetes_df["diabetes"] <- c(as.integer(new_diabetes_df["diabetes"] == "pos"))
new_diabetes_df <- na.omit(new_diabetes_df) # remove all the rows containing missing values
head(new_diabetes_df)
```

After the elimination of all the rows containing missing values I can split the dataset in train and test sets, as I did before.

```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == 1, ]
negatives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == 0, ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- na.omit(train_data[sample(1:nrow(train)), ])
test_data <- na.omit(test_data[sample(1:nrow(test)), ])

```

Now, I apply and fit all the methods, as I did with the complete dataset:

```{r}

fold5_errors <- knn.cv(x = as.matrix(subset(train_data, select = -diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data, select = -diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train_data), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean", type = "C")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))


# plot the results
plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.20, 0.38), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = c("blue", "red", "green"), 
       lty = 1)
```
From the graph, we can notice that now, for k equal to 70 onward, the use of cross-validation does not generate an underestimation of the error anymore: the three lines in the plot have similar values.

```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)
```

```{r}
test_err_5fold_2 <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo_2 <- 1- loo_errors$crit[k_min_loo]
test_err_knn_2 <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold_2)
sprintf('Test error with LOO cv: %s', test_err_loo_2)
sprintf('Test error with kNN: %s', test_err_knn_2)
```

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)
```

```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ glucose + pedigree + s(insulin, k = 5), 
                       data = train_data, family = binomial())
summary(model_gam)

```

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error_2 = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error_2)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error_2 = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error_2)
```

```{r}
set.seed(2)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp = 0), 
                             method = "class")
plotcp(classification_tree)

```

```{r}
classification_tree$cptable
```

```{r}
class_tree_10 <- prune(classification_tree, cp = 0.018315018)
par(mar = c(2, 2, 2, 2))
plot(class_tree_10, main = "Pruned Tree")
text(class_tree_10, cex = 0.5, col = "blue")
```

```{r}
tree_train_error_2 <- mean(predict(class_tree_10, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error_2)

tree_test_error_2 <- mean(predict(class_tree_10, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error_2)
```

```{r}
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error_2 <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error_2)

bagged_test_error_2 <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_test_error_2)
```

```{r}
random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100, mtry=2)
forest_train_error_2 <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                           != train_data$diabetes)
sprintf("Train error Random Forest: %s", forest_train_error_2)

forest_test_error_2 <- mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
                      != test_data$diabetes)
sprintf("Train error Random Forest: %s", forest_test_error_2)
```
After thf fitting of all the methods, it is possible to make a comparison of the new results with the previous ones. The following table show what I obtained.


```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
test_errors_2 <- c(test_err_5fold_2, test_err_loo_2, gam_test_error_2, tree_test_error_2,
                 bagged_test_error_2, forest_test_error_2)
df_comparison <- data.frame(methods, test_errors, test_errors_2)
names(df_comparison) <- c("Method", "Test Error 1", "Test error 2")
df_comparison
```


The results related to the correct dataset, as expected, are better. This is due to the fact that the data I removed were not correct (since they were set to 0 even if they were missing values). Therefore, with the correct data, the performance of almost all the models (except fro GAM and Classification Tree) is improved. In this case, the best model is Random Forest. kNN with LOO and 5-fold cross validation still have a good performance, very close to the one of Random Forest. Bagging has an intermediate position, while GAM and classification trees are the worst ones.
