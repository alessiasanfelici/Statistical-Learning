---
title: "Assignment 2"
output: html_document
date: "2023-10-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
```

## Problem 1: Regression 

bla bla bla

```{r}
df_toxicity <- read.csv("aquatic_toxicity.csv", sep = ";", header = FALSE)
colnames(df_toxicity) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p","nN", "C040", "LC50")
# create the dichotomic version of the dataset (where the count variables are transformed using 0/1 dummy encoding)

df_toxicity_dummy = df_toxicity
df_toxicity_dummy["H050"] = c(as.integer(df_toxicity_dummy ["H050"]>0))
df_toxicity_dummy ["nN"] = c(as.integer(df_toxicity_dummy ["nN"]>0))
df_toxicity_dummy ["C040"] = c(as.integer(df_toxicity_dummy ["C040"]>0))

```


```{r}
set.seed(1)

# create train and test data
sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ]
test_dummy <- df_toxicity_dummy[!sample, ]
```

```{r}
# linear regression model with original dataset
linear_model <- lm(LC50 ~ . , data = train)
summary(linear_model)
```

```{r}
prediction <- predict.lm(linear_model, test)
train_error = mean(linear_model$residuals^2)
test_error = mean((test$LC50 - prediction)^2)
sprintf("Train error: %s", train_error) # round the error??
sprintf("Test error: %s", test_error) # round the error??
```

```{r}
# linear regression model with dummy dataset
linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
summary(linear_model_dummy)
```
```{r}
prediction_dummy = predict.lm(linear_model_dummy, test_dummy)
train_error_dummy = mean(linear_model_dummy$residuals^2)
test_error_dummy = mean((test_dummy$LC50 - prediction_dummy)^2)
sprintf("Train error dummy: %s", train_error_dummy) # round the error??
sprintf("Test error dummy: %s", test_error_dummy) # round the error??
```

```{r}
# repeat the process 200 times 
repetitions = 200
train_error = 0
test_error = 0
train_error_dummy = 0
test_error_dummy = 0

for (i in 1:repetitions){
  sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
  train <- df_toxicity[sample, ]
  test <- df_toxicity[!sample, ]
  train_dummy <- df_toxicity_dummy[sample, ]
  test_dummy <- df_toxicity_dummy[!sample, ]

  linear_model <- lm(LC50 ~ . , data = train)
  prediction <- predict.lm(linear_model, test)
  train_error = train_error + mean(linear_model$residuals^2)/repetitions
  test_error = test_error + mean((test$LC50 - prediction)^2)/repetitions

  linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
  prediction_dummy = predict.lm(linear_model_dummy, test_dummy)
  train_error_dummy = train_error_dummy + mean(linear_model_dummy$residuals^2)/repetitions
  test_error_dummy = test_error_dummy + mean((test_dummy$LC50 -
                                                prediction_dummy)^2)/repetitions

} # NB: perchÃ© divide per repetitions ogni volta e somma?? Pensarci!!!!
```

```{r}
sprintf("Train error: %s", train_error) # round the error??
sprintf("Test error: %s", test_error) # round the error??
sprintf("Train error dummy: %s", train_error_dummy) # round the error??
sprintf("Test error dummy: %s", test_error_dummy) # round the error??
```
# answer to question 1
(Bootstrapping)?

# answer to question 2
classification, as seen in ML

# VARIABLE SELECTION

```{r}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ] # need only the first case here?? Ask!
test_dummy <- df_toxicity_dummy[!sample, ]

full.model <- lm(LC50 ~ . , data = train)
null.model <- lm(LC50 ~ 1, data = train)
```

```{r}
# Backward selection with BIC stopping criterion
backward_bic <- step(full.model, direction = "backward", k = log(nrow(train)), trace = FALSE)
print("Backward selection with BIC stopping criterion")
backward_bic
```

```{r}
# Backward selection with AIC stopping criterion
backward_aic <- step(full.model, direction = "backward", k = 2, trace = FALSE)
print("Backward selection with AIC stopping criterion")
backward_aic
```

```{r}
# Forward selection with BIC stopping criterion
forward_bic <- step(null.model, direction = "forward", scope = formula(full.model), k = log(nrow(train)), trace = FALSE)
print("Forward selection with BIC stopping criterion")
forward_bic
```

```{r}
# Forward selection with AIC stopping criterion
forward_aic <- step(null.model, direction = "forward", scope = formula(full.model), k = 2, trace = FALSE)
print("Forward selection with AIC stopping criterion")
forward_aic
```

Add a comment about the different models!
Add best subset?????

# Ridge Regression

```{r}
library(glmnet)
x = as.matrix(subset(train, select = -LC50))
y = as.vector(train$LC50)
# Create a sequence of lambda values to try
lambda_seq <- 10^seq(-5, 5, length = 100)

ridge_cv_model <- cv.glmnet(x, y, alpha = 0, lambda = lambda_seq, nfolds = 5)

print(ridge_cv_model)

```

```{r}
# Plot the MSE with respect to lambda
plot(ridge_cv_model, ylab = "MSE", xlim = c(-5, 5))
```

```{r}
best_lambda <- ridge_cv_model$lambda[which.min(ridge_cv_model$cvm)]
sprintf("The best value of lambda is %s", best_lambda)
#coef(ridge_cv_model, s = "lambda.min")
```
So, now we compute the train and test error for the model with the best value of lambda.#add cv even here or not???

```{r}
ridge_best <- glmnet(x, y, alpha = 0, lambda = best_lambda)
ridge_cv_train_err = mean((predict(ridge_best, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_cv_test_err = mean((predict(ridge_best, as.matrix(subset(test, select = -LC50))) -
                            as.vector(test$LC50))^2)
sprintf("Ridge cv train error: %s", ridge_cv_train_err)
sprintf("Ridge cv test error: %s", ridge_cv_test_err)
```
MSE???

```{r}
library(boot)
library(pracma)
ridge_fit <- function(data, indices, lambda_seq) {
  data_train <- data[indices, ]
  X_train <- as.matrix(subset(data_train, select = -LC50))
  y_train <- as.vector(data_train$LC50)
  fit <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_seq)
  y_pred_train <- predict(fit, newx = X_train, s = lambda_seq)
  mse <- sum((y_train - y_pred_train)^2) / length(y_train)
  return (mse) 
}

mse_boot = c()
for (i in 1:100){
  lambda = lambda_seq[i]
  boot_results <- boot(data = train, statistic = ridge_fit, R = 100, lambda = lambda)
  mse_boot[i] = mean(boot_results$t)
}
```


```{r}
plot(log10(ridge_cv_model$lambda), ridge_cv_model$cvm, ylab = "MSE", type = "l", xlab = "log(lambda)", col = "red", ylim = c(1, 3))
lines(log10(lambda_seq), mse_boot, col = "black", type = "l", ylim = c(1, 3))
legend("topleft", legend = c("Cross-Validation MSE", "Bootstrap MSE"), col = c("red", "black"), lty = 1)

```

```{r}
best_lambda_boot <- lambda_seq[which.min(mse_boot)]
sprintf("The best value of lambda is %s", best_lambda_boot)
```
So, now we compute the train and test error for the model with the best value of lambda.#add bootstrap even here or not???

```{r}
ridge_best_boot <- glmnet(x, y, alpha = 0, lambda = best_lambda_boot)
ridge_boot_train_err = mean((predict(ridge_best_boot, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_boot_test_err = mean((predict(ridge_best_boot, as.matrix(subset(test, 
                                    select = -LC50))) - as.vector(test$LC50))^2)
sprintf("Ridge bootstrap train error: %s", ridge_boot_train_err)
sprintf("Ridge bootstrap test error: %s", ridge_boot_test_err)

```