---
title: "Assignment 2"
output: html_document
date: "2023-10-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
```

## Problem 1: Regression 

bla bla bla

```{r}
df_toxicity <- read.csv("aquatic_toxicity.csv", sep = ";", header = FALSE)
colnames(df_toxicity) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p","nN", "C040", "LC50")
# create the dichotomic version of the dataset (where the count variables are transformed using 0/1 dummy encoding)

df_toxicity_dummy <- df_toxicity
df_toxicity_dummy["H050"] <- c(as.integer(df_toxicity_dummy ["H050"]>0))
df_toxicity_dummy ["nN"] <- c(as.integer(df_toxicity_dummy ["nN"]>0))
df_toxicity_dummy ["C040"] <- c(as.integer(df_toxicity_dummy ["C040"]>0))

```


```{r}
set.seed(1)

# create train and test data
sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ]
test_dummy <- df_toxicity_dummy[!sample, ]
```

```{r}
# linear regression model with original dataset
linear_model <- lm(LC50 ~ . , data = train)
summary(linear_model)
```

```{r}
prediction <- predict.lm(linear_model, test)
train_error <- mean(linear_model$residuals^2)
test_error <- mean((test$LC50 - prediction)^2)
sprintf("Train error: %s", train_error) 
sprintf("Test error: %s", test_error) 
```

```{r}
# linear regression model with dummy dataset
linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
summary(linear_model_dummy)
```
```{r}
prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
train_error_dummy <- mean(linear_model_dummy$residuals^2)
test_error_dummy <- mean((test_dummy$LC50 - prediction_dummy)^2)
sprintf("Train error dummy: %s", train_error_dummy) 
sprintf("Test error dummy: %s", test_error_dummy)
```

```{r}
# repeat the process 200 times 
repetitions <- 200
train_error <- 0
test_error <- 0
train_error_dummy <- 0
test_error_dummy <- 0

for (i in 1:repetitions){
  sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
  train <- df_toxicity[sample, ]
  test <- df_toxicity[!sample, ]
  train_dummy <- df_toxicity_dummy[sample, ]
  test_dummy <- df_toxicity_dummy[!sample, ]

  linear_model <- lm(LC50 ~ . , data = train)
  prediction <- predict.lm(linear_model, test)
  train_error <- train_error + mean(linear_model$residuals^2)/repetitions
  test_error <- test_error + mean((test$LC50 - prediction)^2)/repetitions

  linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
  prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
  train_error_dummy <- train_error_dummy + mean(linear_model_dummy$residuals^2)/repetitions
  test_error_dummy <- test_error_dummy + mean((test_dummy$LC50 -
                                                prediction_dummy)^2)/repetitions

} # NB: perchÃ© divide per repetitions ogni volta e somma?? Pensarci!!!!
```

```{r}
sprintf("Train error: %s", train_error) # round the error??
sprintf("Test error: %s", test_error) # round the error??
sprintf("Train error dummy: %s", train_error_dummy) # round the error??
sprintf("Test error dummy: %s", test_error_dummy) # round the error??
```
# answer to question 1
(Bootstrapping)?

# answer to question 2
classification, as seen in ML

# VARIABLE SELECTION

```{r}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ] # need only the first case here?? Ask!
test_dummy <- df_toxicity_dummy[!sample, ]

full.model <- lm(LC50 ~ . , data = train)
null.model <- lm(LC50 ~ 1, data = train)
```

```{r}
# Backward selection with BIC stopping criterion
backward_bic <- step(full.model, direction = "backward", k = log(nrow(train)), trace = FALSE)
print("Backward selection with BIC stopping criterion")
backward_bic
```

```{r}
# Backward selection with AIC stopping criterion
backward_aic <- step(full.model, direction = "backward", k = 2, trace = FALSE)
print("Backward selection with AIC stopping criterion")
backward_aic
```

```{r}
# Forward selection with BIC stopping criterion
forward_bic <- step(null.model, direction = "forward", scope = formula(full.model), k = log(nrow(train)), trace = FALSE)
print("Forward selection with BIC stopping criterion")
forward_bic
```

```{r}
# Forward selection with AIC stopping criterion
forward_aic <- step(null.model, direction = "forward", scope = formula(full.model), k = 2, trace = FALSE)
print("Forward selection with AIC stopping criterion")
forward_aic
```

Add a comment about the different models!
Add best subset?????

# Ridge Regression

```{r}
library(glmnet)
x <- as.matrix(subset(train, select = -LC50))
y <- as.vector(train$LC50)
# Create a sequence of lambda values to try
lambda_seq <- 10^seq(-5, 5, length = 100)

ridge_cv_model <- cv.glmnet(x, y, alpha = 0, lambda = lambda_seq, nfolds = 5)

print(ridge_cv_model)

```

```{r}
# Plot the MSE with respect to lambda
plot(ridge_cv_model, ylab = "MSE", xlim = c(-5, 5))
```

```{r}
best_lambda <- ridge_cv_model$lambda[which.min(ridge_cv_model$cvm)]
sprintf("The best value of lambda is %s", best_lambda)
```
So, now we compute the train and test error for the model with the best value of lambda.#add cv even here or not???

```{r}
ridge_best <- glmnet(x, y, alpha = 0, lambda = best_lambda)
ridge_cv_train_err <- mean((predict(ridge_best, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_cv_test_err <- mean((predict(ridge_best, as.matrix(subset(test, select = -LC50))) -
                            as.vector(test$LC50))^2)
sprintf("Ridge cv train error: %s", ridge_cv_train_err)
sprintf("Ridge cv test error: %s", ridge_cv_test_err)
```
MSE???

```{r}
library(boot)
library(pracma)
ridge_fit <- function(data, indices, lambda_seq) {
  data_train <- data[indices, ]
  X_train <- as.matrix(subset(data_train, select = -LC50))
  y_train <- as.vector(data_train$LC50)
  fit <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_seq)
  y_pred_train <- predict(fit, newx = X_train, s = lambda_seq)
  mse <- sum((y_train - y_pred_train)^2) / length(y_train)
  return (mse) 
}

mse_boot <- c()
for (i in 1:100){
  lambda = lambda_seq[i]
  boot_results <- boot(data = train, statistic = ridge_fit, R = 100, lambda = lambda)
  mse_boot[i] = mean(boot_results$t)
}
```


```{r}
plot(log10(ridge_cv_model$lambda), ridge_cv_model$cvm, ylab = "MSE", type = "l", xlab = "log(lambda)", col = "red", ylim = c(1, 3))
lines(log10(lambda_seq), mse_boot, col = "black", type = "l", ylim = c(1, 3))
legend("topleft", legend = c("5-fold Cross-Validation", "Bootstrap"), col = c("red", "black"), lty = 1)

```

```{r}
best_lambda_boot <- lambda_seq[which.min(mse_boot)]
sprintf("The best value of lambda is %s", best_lambda_boot)
```
So, now we compute the train and test error for the model with the best value of lambda.#add bootstrap even here or not???

```{r}
ridge_best_boot <- glmnet(x, y, alpha = 0, lambda = best_lambda_boot)
ridge_boot_train_err <- mean((predict(ridge_best_boot, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_boot_test_err <- mean((predict(ridge_best_boot, as.matrix(subset(test, 
                                    select = -LC50))) - as.vector(test$LC50))^2)
sprintf("Ridge bootstrap train error: %s", ridge_boot_train_err)
sprintf("Ridge bootstrap test error: %s", ridge_boot_test_err)

```
GAM

```{r}
library(gam)

degrees = 3:10

train_error_gam <- zeros(length(degrees), 1)
test_error_gam <- zeros(length(degrees), 1)

for (k in degrees){
  gam_model <- gam(LC50 ~ s(TPSA, df = k) + s(SAacc, df = k) + s(H050, df = k) + 
                     s(MLOGP, df = k) + s(RDCHI, df = k) + s(GATS1p, df = k) + 
                     s(nN, df = k) + s(C040, df = k), data = train)
  
  train_error_gam[k-2] <- mean((predict(gam_model, subset(train, select = -LC50)) -
                                 train$LC50)^2)
  test_error_gam[k-2] <- mean((predict(gam_model, subset(test, select = -LC50)) -
                                 test$LC50)^2)
}


```


```{r}
df_gam <- data.frame(degrees, train_error_gam, test_error_gam)
names(df_gam) <- c("Degrees of Freedom", "Train Error", "Test Error")
df_gam

```
Best ones = small degrees.

REGRESSION TREE

```{r}
library(rpart)

# Grow the full tree
full_tree <- rpart(LC50 ~ . , data = train, control = rpart.control(cp=0))
plotcp(full_tree)

```


```{r}
full_tree$cptable

```
The lowest error is given y a cp = 0.0117017714.

```{r}
# Prune the tree
pruned_tree <- prune(full_tree, cp = 0.0117017714)
par(mar = c(2, 2, 2, 2))
plot(pruned_tree, main = "Pruned Tree")
text(pruned_tree, cex = 0.5, col = "blue")

```


```{r}
# Train and test error for the pruned tree

train_error_tree <- mean((predict(pruned_tree, subset(train, select = -LC50)) -
                           train$LC50)^2)
test_error_tree <- mean((predict(pruned_tree, subset(test, select = -LC50)) - 
                          test$LC50)^2)
sprintf("Train error Regression Tree: %s", train_error_tree)
sprintf("Test error Regression Tree: %s", test_error_tree)
```
COMPARISON OF ALL THE METHODS

```{r}
methods <- c("Linear Regression", "Dummy Linear Regression", "Variable selection",
            "Ridge Cross-Validation", "Ridge Bootstrap", "GAM", "Regression Tree")
train_errors <- c(train_error, train_error_dummy, mean(backward_bic$residuals^2),
                 ridge_cv_train_err, ridge_boot_train_err, train_error_gam[1],
                 train_error_tree)
test_errors <- c(test_error, test_error_dummy, mean((test$LC50 - 
                                                     predict.lm(backward_bic, test))^2),
               ridge_cv_test_err, ridge_boot_test_err, test_error_gam[1],
               test_error_tree)
df_comparison <- data.frame(methods, train_errors, test_errors)
names(df_comparison) <- c("Method", "Train Error", "Test Error")
df_comparison

```

The above table shows the train and the test errors for all the different methods used by now. If we consider the best method as the one that minimizes the test error, then in this case the best one is Linear Regression. All the other methods give a larger test error.
We can also see that GAM and Regression Tree have a lower train error but a larger test error with respect to the other methods. This means that these techniques work very well with the train sample, but when dealing with new data (test sample) their performance is worse, thus the test error is larger. This behavior suggest a problem of overfitting for these two methodologies. 


## Problem 2: Classification 

```{r}
library(mlbench)

data(PimaIndiansDiabetes)
diabetes_df <- as.data.frame(PimaIndiansDiabetes)
diabetes_df["diabetes"] <- c(as.integer(diabetes_df["diabetes"] == "pos"))
diabetes_df

```


```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- diabetes_df[diabetes_df["diabetes"] == "1", ]
negatives_df <- diabetes_df[diabetes_df["diabetes"] == "0", ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- train_data[sample(1:nrow(train)), ]
test_data <- test_data[sample(1:nrow(test)), ]
```

kNN

```{r}
library(class)
library(cvms)
library(kknn)
library(Rfast)

fold5_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))

plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.25, 0.45), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = c("blue", "red", "green"), 
       lty = 1)

```
Cross-validation tends to underestimate the error, especially for small values of k.

```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)

```


```{r}
test_err_5fold <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo <- 1- loo_errors$crit[k_min_loo]
test_err_knn <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold)
sprintf('Test error with LOO cv: %s', test_err_loo)
sprintf('Test error with kNN: %s', test_err_knn)

```
GAM

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```
 looking at the edf column (effective
degrees of freedom), we can see that the pressure does not show any non-linear behaviour, so that variable
will be considered linear (change!!!) ??????

```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ s(glucose, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```


```{r}
par(mfrow = c(2,2))
plot(model_gam)

```

Comment:

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error)
```

Classification tree

```{r}
set.seed(1)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp=0), 
                             method = "class")
plotcp(classification_tree)

```

min value of cp = 0.010582011 is related to size of 10

```{r}

classification_tree$cptable


class_tree_10 <- prune(classification_tree, cp = 0.010582011)
par(mar = c(2, 2, 2, 2))
plot(class_tree_10, main = "Pruned Tree")
text(class_tree_10, cex = 0.5, col = "blue")

```


```{r}
tree_train_error <- mean(predict(class_tree_10, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error)

tree_test_error <- mean(predict(class_tree_10, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error)

```
Bagged trees


```{r}
bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error)

bagged_test_error <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_test_error)

```
Random forest

```{r}
library(randomForest)
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100)
forest_train_error <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                           != train_data$diabetes)
sprintf("Train error Random Forest: %s", forest_train_error)

forest_test_error <- mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
                        != test_data$diabetes)
sprintf("Train error Random Forest: %s", forest_test_error)

```
Comparison

```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "kNN", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, test_err_knn, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
df_comparison <- data.frame(methods, test_errors)
names(df_comparison) <- c("Method", "Test Error")
df_comparison
```

Comment:


New Dataset (correct)

```{r}
data(PimaIndiansDiabetes2)
new_diabetes_df <- as.data.frame(PimaIndiansDiabetes2)
new_diabetes_df["diabetes"] <- c(as.integer(new_diabetes_df["diabetes"] == "pos"))
new_diabetes_df
```


```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == "1", ]
negatives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == "0", ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- train_data[sample(1:nrow(train)), ]
test_data <- test_data[sample(1:nrow(test)), ]

```

```{r}
fold5_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))

plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.35, 0.55), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = c("blue", "red", "green"), 
       lty = 1)
```


```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)
```

```{r}
test_err_5fold_2 <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo_2 <- 1- loo_errors$crit[k_min_loo]
test_err_knn_2 <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold_2)
sprintf('Test error with LOO cv: %s', test_err_loo_2)
sprintf('Test error with kNN: %s', test_err_knn_2)
```

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)
```


```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ s(glucose, k = 5) + s(pedigree, k = 5) + s(age, k = 5), 
                       data = train_data, family = binomial())
summary(model_gam)

```

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error_2 = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error_2)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error_2 = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error_2)
```


```{r}
set.seed(2)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp = 0), 
                             method = "class")
plotcp(classification_tree)

```

```{r}
classification_tree$cptable


class_tree_10 <- prune(classification_tree, cp = 0.015873016)
par(mar = c(2, 2, 2, 2))
plot(class_tree_10, main = "Pruned Tree")
text(class_tree_10, cex = 0.5, col = "blue")
```

```{r}
tree_train_error_2 <- mean(predict(class_tree_10, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error_2)

tree_test_error_2 <- mean(predict(class_tree_10, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error_2)
```


```{r}
bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error_2 <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error_2)

bagged_test_error_2 <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_test_error_2)
```

Sistemare: !!!!!!!

```{r}
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

#random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100, mtry=2)
#forest_train_error_2 <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                #           != train_data$diabetes)
#sprintf("Train error Random Forest: %s", forest_train_error_2)

forest_test_error_2 <- 1 #mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
   #                     != test_data$diabetes)
sprintf("Train error Random Forest: %s", forest_test_error_2)
```

```{r}

```

```{r}

```

```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "kNN", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, test_err_knn, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
test_errors_2 <- c(test_err_5fold_2, test_err_loo_2, test_err_knn_2, gam_test_error_2, tree_test_error_2,
                 bagged_test_error_2, forest_test_error_2)
df_comparison <- data.frame(methods, test_errors, test_errors_2)
names(df_comparison) <- c("Method", "Test Error 1", "Test error 2")
df_comparison
```


```{r}

```


```{r}

```


```{r}

```

