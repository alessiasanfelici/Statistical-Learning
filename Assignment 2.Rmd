---
title: "Assignment 2"
output: html_document
date: "2023-10-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
```

## Problem 1: Regression 

The first problem of the assignment consists in the analysis of data coming from a study on data toxicity. The dataset is composed of 546 observations and 9 variables. The aim of the analysis is to predict predict acute aquatic toxicity, which is measured through a variable called $LC50$ (the concentration that causes death in 50% of the planktonic crustacean $Daphnia magna$ over a test duration of 48 hours). The other 8 variables, used for the prediction, are:
- $TPSA$: the topological polar surface area calculated by means of a contribution method that takes into account nitrogen, oxygen, potassium and sulphur;
- $SAacc$: the Van der Waals surface area (VSA) of atoms that are acceptors of hydrogen bonds;
- $H050$: the number of hydrogen atoms bonded to heteroatoms;
- $MLOGP$: expresses the lipophilicity of a molecule, this being the driving force of narcosis;
- $RDCHI$: a topological index that encodes information about molecular size and branching;
- $GATS1p$: information on molecular polarisability;
- $nN$: the number of nitrogen atoms present in the molecule;
- $C040$: the number of carbon atoms of a certain type, including esters, carboxylic acids, thioesters, carbamic acids, nitriles, etc.;

```{r}
df_toxicity <- read.csv("aquatic_toxicity.csv", sep = ";", header = FALSE)
colnames(df_toxicity) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p","nN", "C040", "LC50")
# create the dichotomic version of the dataset (where the count variables are transformed using 0/1 dummy encoding)

df_toxicity_dummy <- df_toxicity
df_toxicity_dummy["H050"] <- c(as.integer(df_toxicity_dummy ["H050"]>0))
df_toxicity_dummy ["nN"] <- c(as.integer(df_toxicity_dummy ["nN"]>0))
df_toxicity_dummy ["C040"] <- c(as.integer(df_toxicity_dummy ["C040"]>0))

```

# Linear Regression
Firstly, I want to compare the results of the application of linear regression with both the original dataset and a modified dataset, where all the count variables have been transformed using a 0/1 dummy encoding where 0 represents absence of the specific atom and 1 represents presence of the specific atoms. for this reason, I have already created (above) a dicothomic version of the dataset.

Now, I can divide both the datasets into train and test sets, containing 70% and 30% of data, respectively, and then fit linear regression in the two cases and compare the obtained results.

```{r}
set.seed(1)

# create train and test data
sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]
train_dummy <- df_toxicity_dummy[sample, ]
test_dummy <- df_toxicity_dummy[!sample, ]
```

```{r}
# linear regression model with original dataset
linear_model <- lm(LC50 ~ . , data = train)
linear_model
```

```{r}
prediction <- predict.lm(linear_model, test)
train_error <- mean(linear_model$residuals^2)
test_error <- mean((test$LC50 - prediction)^2)
sprintf("Train error: %s", train_error) 
sprintf("Test error: %s", test_error) 
```

```{r}
# linear regression model with dummy dataset
linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
linear_model_dummy
```
```{r}
prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
train_error_dummy <- mean(linear_model_dummy$residuals^2)
test_error_dummy <- mean((test_dummy$LC50 - prediction_dummy)^2)
sprintf("Train error dummy: %s", train_error_dummy) 
sprintf("Test error dummy: %s", test_error_dummy)
```
The coefficients, in the two cases, have different meanings. If for dummy variables they represent the effect that the presence of the atoms has on the target variable, when dealing with linear values, they represent the change in the dependent variable associated with a one-unit change in that continuous predictor while holding all other predictors constant.

Moreover, we can easily notice that the test error associated to the original dataset is smaller than the one related to the dichotomic dataset.

I then repeat the same procedure 200 times, such that each time I create a new training/test split and I apply linear regression to both the versions of the dataset. The idea behind the repetition is the following: it allows to assess the stability and robustness of the models across different data samples. This repeated process helps you account for the potential variability in model performance due to the randomness in the split of data. In addition, it also provides more reliable estimates of the average performance of the model.

```{r}
# repeat the process 200 times 
repetitions <- 200
train_error <- 0
test_error <- 0
test_errors_lr <- zeros(repetitions, 1)
train_error_dummy <- 0
test_error_dummy <- 0
test_errors_dummy_lr <- zeros(repetitions, 1)

for (i in 1:repetitions){
  sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
  train <- df_toxicity[sample, ]
  test <- df_toxicity[!sample, ]
  train_dummy <- df_toxicity_dummy[sample, ]
  test_dummy <- df_toxicity_dummy[!sample, ]

  linear_model <- lm(LC50 ~ . , data = train)
  prediction <- predict.lm(linear_model, test)
  train_error <- train_error + mean(linear_model$residuals^2)/repetitions
  test_error <- test_error + mean((test$LC50 - prediction)^2)/repetitions
  test_errors_lr[i-1] <- mean((test$LC50 - prediction)^2)/repetitions

  linear_model_dummy <- lm(LC50 ~ ., data = train_dummy)
  prediction_dummy <- predict.lm(linear_model_dummy, test_dummy)
  train_error_dummy <- train_error_dummy + mean(linear_model_dummy$residuals^2)/repetitions
  test_error_dummy <- test_error_dummy + mean((test_dummy$LC50 -
                                                 prediction_dummy)^2)/repetitions
  test_errors_dummy_lr[i-1] <- mean((test_dummy$LC50 - prediction_dummy)^2)/repetitions

}

```

```{r}
boxplot(list(test_errors_lr, test_errors_dummy_lr), col = c("red", "blue"), xlab = "Dataset", ylab = "Test Errors")

legend("bottomleft", legend = c("Original dataset", "Dichotomic dataset"), fill = c("red", "blue"))
```

```{r}
sprintf("Train error: %s", train_error) 
sprintf("Test error: %s", test_error) 
sprintf("Train error dummy: %s", train_error_dummy) 
sprintf("Test error dummy: %s", test_error_dummy) 
```
As it is possible to can see from both the plot and the results of the errors, in general, the use of the dataset with dicothomic count variables produces worse results. This is due to the fact that using dummy variables is an oversimplification and thus valuable information is ignored.

Due to this result, in the next steps I'm going to focus only on the original dataset, without taking into account the dichotomic dataset.


# Variable selection
The next step consists in comparing different variable selection procedures with different stopping criteria. In particular, I'm focusing on Backward Elimination and Forward Selection, both tested with AIC and BIC.

```{r}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(df_toxicity), replace = TRUE, prob = c(0.7, 0.3))
train <- df_toxicity[sample, ]
test <- df_toxicity[!sample, ]

full.model <- lm(LC50 ~ . , data = train)
null.model <- lm(LC50 ~ 1, data = train)
```

```{r}
# Backward elimination with BIC stopping criterion
backward_bic <- step(full.model, direction = "backward", k = log(nrow(train)), trace = FALSE)
print("Backward elimination with BIC stopping criterion")
backward_bic
```

```{r}
# Backward elimination with AIC stopping criterion
backward_aic <- step(full.model, direction = "backward", k = 2, trace = FALSE)
print("Backward elimination with AIC stopping criterion")
backward_aic
```

```{r}
# Forward selection with BIC stopping criterion
forward_bic <- step(null.model, direction = "forward", scope = formula(full.model), k = log(nrow(train)), trace = FALSE)
print("Forward selection with BIC stopping criterion")
forward_bic
```

```{r}
# Forward selection with AIC stopping criterion
forward_aic <- step(null.model, direction = "forward", scope = formula(full.model), k = 2, trace = FALSE)
print("Forward selection with AIC stopping criterion")
forward_aic
```
As shown from the results of the 4 methods, the resulting models are the same in all the cases. All of them contain the same 6 variables (the coefficients are different, but the variables are the same): TPSA, SAacc, ML0GP, RDCHI, GATS1p and nN.

# Ridge Regression
Now, I study the performance of ridge regression, using both 5-fold cross-validation and a bootstrap procedure (considering 100 bootstrap iterations) . The idea is to find the optimal complexity parameter among a grid of lambda values.

```{r}
library(glmnet)
x <- as.matrix(subset(train, select = -LC50))
y <- as.vector(train$LC50)
# Create a sequence of lambda values to try
lambda_seq <- 10^seq(-5, 5, length = 100)

# 5-fold cross-validation
ridge_cv_model <- cv.glmnet(x, y, alpha = 0, lambda = lambda_seq, nfolds = 5)

print(ridge_cv_model)

```

```{r}
# Plot the MSE with respect to lambda
plot(ridge_cv_model, ylab = "MSE", xlim = c(-5, 5))
```

```{r}
best_lambda <- ridge_cv_model$lambda[which.min(ridge_cv_model$cvm)]
sprintf("The best value of lambda is %s", best_lambda)
```
So, I compute the train and test error for the model with the optimal value of lambda:

```{r}
ridge_best <- glmnet(x, y, alpha = 0, lambda = best_lambda)
ridge_cv_train_err <- mean((predict(ridge_best, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_cv_test_err <- mean((predict(ridge_best, as.matrix(subset(test, select = -LC50))) -
                            as.vector(test$LC50))^2)
sprintf("Ridge cv train error: %s", ridge_cv_train_err)
sprintf("Ridge cv test error: %s", ridge_cv_test_err)
```

```{r}
library(boot)
library(pracma)

# Bootstrap
ridge_fit <- function(data, indices, lambda_seq) {
  data_train <- data[indices, ]
  X_train <- as.matrix(subset(data_train, select = -LC50))
  y_train <- as.vector(data_train$LC50)
  fit <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_seq)
  y_pred_train <- predict(fit, newx = X_train, s = lambda_seq)
  mse <- sum((y_train - y_pred_train)^2) / length(y_train)
  return (mse) 
}

mse_boot <- c()
for (i in 1:100){
  lambda = lambda_seq[i]
  boot_results <- boot(data = train, statistic = ridge_fit, R = 100, lambda = lambda)
  mse_boot[i] = mean(boot_results$t)
}
```


```{r}
plot(log10(ridge_cv_model$lambda), ridge_cv_model$cvm, ylab = "MSE", type = "l", xlab = "log(lambda)", col = "red", ylim = c(1.2, 3))
lines(log10(lambda_seq), mse_boot, col = "black", type = "l", ylim = c(1, 3))
legend("topleft", legend = c("5-fold Cross-Validation", "Bootstrap"), col = c("red", "black"), lty = 1)

```

```{r}
best_lambda_boot <- lambda_seq[which.min(mse_boot)]
sprintf("The best value of lambda is %s", best_lambda_boot)
```
So, I compute the train and test error for the model with the optimal value of lambda:

```{r}
ridge_best_boot <- glmnet(x, y, alpha = 0, lambda = best_lambda_boot)
ridge_boot_train_err <- mean((predict(ridge_best_boot, as.matrix(subset(train, select = -LC50))) -
                             as.vector(train$LC50))^2)
ridge_boot_test_err <- mean((predict(ridge_best_boot, as.matrix(subset(test, 
                                    select = -LC50))) - as.vector(test$LC50))^2)
sprintf("Ridge bootstrap train error: %s", ridge_boot_train_err)
sprintf("Ridge bootstrap test error: %s", ridge_boot_test_err)

```
The train and test errors resulting form the two methods are very similar to each other, especially the test error. This means that the two methods work similarly, as it is also shown in the figure above, which allows to compare the MSE w.r.t lambda in the two cases.


# Generalized Additive Model

Next, I consider possible non linear effects, fitting a generalized additive model  in which the effects of the variables are fitted using smoothing splines. I decided to compare different levels of complexity for the smoothing splines. In particular, I focused on degrees of freedom from 3 to 10.

```{r}
library(gam)

degrees = 3:10

train_error_gam <- zeros(length(degrees), 1)
test_error_gam <- zeros(length(degrees), 1)

# Fit GAM and compute the errors for each degree of freedom
for (k in degrees){
  gam_model <- gam(LC50 ~ s(TPSA, df = k) + s(SAacc, df = k) + s(H050, df = k) + 
                     s(MLOGP, df = k) + s(RDCHI, df = k) + s(GATS1p, df = k) + 
                     s(nN, df = k) + s(C040, df = k), data = train)
  
  train_error_gam[k-2] <- mean((predict(gam_model, subset(train, select = -LC50)) -
                                 train$LC50)^2)
  test_error_gam[k-2] <- mean((predict(gam_model, subset(test, select = -LC50)) -
                                 test$LC50)^2)
}


```


```{r}
# Create a dataframe containing the errors for each degree of freedom
df_gam <- data.frame(degrees, train_error_gam, test_error_gam)
names(df_gam) <- c("Degrees of Freedom", "Train Error", "Test Error")
df_gam

```

I can immediately notice, from the above table, that the lowest test errors are related to small degrees of freedom. In particular, for the final comparison, I will store the result related to degree 3, which correspond to the lowest value of test error.

# Regression Tree

```{r}
library(rpart)

set.seed(1)

# Grow the full tree
full_tree <- rpart(LC50 ~ . , data = train, control = rpart.control(cp=0))
plotcp(full_tree)

```


```{r}
full_tree$cptable

```
The lowest error is given y a cp = 0.0116540354. So, I prune the tree according to this value. This is the obtained tree:

```{r}
# Prune the tree
pruned_tree <- prune(full_tree, cp = 0.0116540354)
par(mar = c(1, 1, 1, 1))
plot(pruned_tree, main = "Pruned Tree")
text(pruned_tree, cex = 0.5, col = "blue")

```
Then, I compute the train and the test error for the pruned tree:

```{r}
# Train and test error for the pruned tree

train_error_tree <- mean((predict(pruned_tree, subset(train, select = -LC50)) -
                           train$LC50)^2)
test_error_tree <- mean((predict(pruned_tree, subset(test, select = -LC50)) - 
                          test$LC50)^2)
sprintf("Train error Regression Tree: %s", train_error_tree)
sprintf("Test error Regression Tree: %s", test_error_tree)
```
# Comparison of all the methods

Now that I have applied all the methods, I can make a comparison between all of them, in order to draw some conclusions.
```{r}
methods <- c("Linear Regression", "Dummy Linear Regression", "Variable selection",
            "Ridge Cross-Validation", "Ridge Bootstrap", "GAM", "Regression Tree")
train_errors <- c(train_error, train_error_dummy, mean(backward_bic$residuals^2),
                 ridge_cv_train_err, ridge_boot_train_err, train_error_gam[1],
                 train_error_tree)
test_errors <- c(test_error, test_error_dummy, mean((test$LC50 - 
                                                     predict.lm(backward_bic, test))^2),
               ridge_cv_test_err, ridge_boot_test_err, test_error_gam[1],
               test_error_tree)
# Dataframe containing all the errors related to each method
df_comparison <- data.frame(methods, train_errors, test_errors)
names(df_comparison) <- c("Method", "Train Error", "Test Error")
df_comparison

```

The above table shows the train and the test errors for all the different methods used by now. If we consider the best method as the one that minimizes the test error, then in this case the best one is Linear Regression. All the other methods give a larger test error.
We can also see that GAM and Regression Tree have a lower train error but a larger test error with respect to the other methods. This means that these techniques work very well with the train sample, but when dealing with new data (the test sample) their performance is worse, thus the test error is larger. This behavior suggest a problem of overfitting for these two methodologies. 


## Problem 2: Classification 

```{r}
library(mlbench)

data(PimaIndiansDiabetes)
diabetes_df <- as.data.frame(PimaIndiansDiabetes)
diabetes_df["diabetes"] <- c(as.integer(diabetes_df["diabetes"] == "pos"))
diabetes_df

```


```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- diabetes_df[diabetes_df["diabetes"] == "1", ]
negatives_df <- diabetes_df[diabetes_df["diabetes"] == "0", ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- train_data[sample(1:nrow(train)), ]
test_data <- test_data[sample(1:nrow(test)), ]
```

kNN

```{r}
library(class)
library(cvms)
library(kknn)
library(Rfast)

fold5_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))

plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.25, 0.45), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = c("blue", "red", "green"), 
       lty = 1)

```
Cross-validation tends to underestimate the error, especially for small values of k.

```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)

```


```{r}
test_err_5fold <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo <- 1- loo_errors$crit[k_min_loo]
test_err_knn <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold)
sprintf('Test error with LOO cv: %s', test_err_loo)
sprintf('Test error with kNN: %s', test_err_knn)

```
GAM

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```
 looking at the edf column (effective
degrees of freedom), we can see that the pressure does not show any non-linear behaviour, so that variable
will be considered linear (change!!!) ??????

```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ s(glucose, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)

```


```{r}
par(mfrow = c(2,2))
plot(model_gam)

```

Comment:

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error)
```

Classification tree

```{r}
set.seed(1)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp=0), 
                             method = "class")
plotcp(classification_tree)

```

min value of cp = 0.010582011 is related to size of 10

```{r}

classification_tree$cptable


class_tree_10 <- prune(classification_tree, cp = 0.010582011)
par(mar = c(2, 2, 2, 2))
plot(class_tree_10, main = "Pruned Tree")
text(class_tree_10, cex = 0.5, col = "blue")

```


```{r}
tree_train_error <- mean(predict(class_tree_10, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error)

tree_test_error <- mean(predict(class_tree_10, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error)

```
Bagged trees


```{r}
bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error)

bagged_test_error <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_test_error)

```
Random forest

```{r}
library(randomForest)
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100)
forest_train_error <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                           != train_data$diabetes)
sprintf("Train error Random Forest: %s", forest_train_error)

forest_test_error <- mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
                        != test_data$diabetes)
sprintf("Train error Random Forest: %s", forest_test_error)

```
Comparison

```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "kNN", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, test_err_knn, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
df_comparison <- data.frame(methods, test_errors)
names(df_comparison) <- c("Method", "Test Error")
df_comparison
```

Comment:


New Dataset (correct)

```{r}
data(PimaIndiansDiabetes2)
new_diabetes_df <- as.data.frame(PimaIndiansDiabetes2)
new_diabetes_df["diabetes"] <- c(as.integer(new_diabetes_df["diabetes"] == "pos"))
new_diabetes_df
```


```{r}
# Split in train and test sets (we want similar class distributions in the two sets)

positives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == "1", ]
negatives_df <- new_diabetes_df[new_diabetes_df["diabetes"] == "0", ]

set.seed(1)

sample_positives <- sample(c(TRUE, FALSE), nrow(positives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_positives <- positives_df[sample_positives, ]
test_positives <- positives_df[!sample_positives, ]

sample_negatives <- sample(c(TRUE, FALSE), nrow(negatives_df), replace = TRUE, 
                           prob = c(0.7, 0.3))
train_negatives <- negatives_df[sample_negatives, ]
test_negatives <- negatives_df[!sample_negatives, ]

# Reunite train and test data
train_data <- rbind(train_positives, train_negatives)
test_data <- rbind(test_positives, test_negatives)

# Randomly shuffle train and test sets
train_data <- train_data[sample(1:nrow(train)), ]
test_data <- test_data[sample(1:nrow(test)), ]

```

```{r}
fold5_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                      nfolds = 5, stratified = TRUE, seed = TRUE, k = c(1:150),
                      dist.type = "euclidean", type = "C")
loo_errors <- knn.cv(x = as.matrix(subset(train_data,select=-diabetes)), y = train_data$diabetes, 
                    nfolds = nrow(train), stratified = TRUE, seed = TRUE, k = c(1:150),
                    dist.type = "euclidean")
predictions_knn <- knn(xnew = as.matrix(subset(test_data, select = -diabetes)), y = train_data$diabetes,
                  x = as.matrix(subset(train_data, select = -diabetes)), k = c(1:150), type = "C")
test_errors_knn <- colMeans(abs(predictions_knn - test_data$diabetes))

plot(1:150, 1 - fold5_errors$crit, type = "l", ylim = c(0.35, 0.55), col = "blue", xlab = "k", 
     ylab = "Error")
lines(1:150, 1 - loo_errors$crit, col = "red")
lines(1:150, test_errors_knn, col = "green")
title("Comparing error estimates")
legend("bottomright", legend = c("5-fold cv", "LOO cv", "Test Errors"), col = c("blue", "red", "green"), 
       lty = 1)
```


```{r}
k_min_5fold <- which.min(1 - fold5_errors$crit)
k_min_loo <- which.min(1 - loo_errors$crit)
k_min_knn <- which.min(test_errors_knn)

sprintf("k with 5-fold cv: %s", k_min_5fold)
sprintf("k with LOO cv: %s", k_min_loo)
sprintf("k with kNN: %s", k_min_knn)
```

```{r}
test_err_5fold_2 <- 1 - fold5_errors$crit[k_min_5fold]
test_err_loo_2 <- 1- loo_errors$crit[k_min_loo]
test_err_knn_2 <- test_errors_knn[k_min_knn]
sprintf('Test error with 5-fold cv: %s', test_err_5fold_2)
sprintf('Test error with LOO cv: %s', test_err_loo_2)
sprintf('Test error with kNN: %s', test_err_knn_2)
```

```{r}
model_gam <- mgcv::gam(diabetes ~ s(pregnant, k = 5) + s(glucose, k = 5) + s(pressure, k = 5) + 
                   s(triceps, k = 5) + s(insulin, k = 5) + s(mass, k = 5) + s(pedigree, k = 5) + 
                   s(age, k = 5), data = train_data, family = binomial())
summary(model_gam)
```


```{r}
# Remove the non significant variables
model_gam <- mgcv::gam(diabetes ~ s(glucose, k = 5) + s(pedigree, k = 5) + s(age, k = 5), 
                       data = train_data, family = binomial())
summary(model_gam)

```

```{r}
ypredict_train <- predict(model_gam, newdata = subset(train_data, select = -diabetes), type = "response")
ypredict_train <- as.factor(as.integer(ypredict_train >= 0.5))
gam_train_error_2 = mean((ypredict_train != train_data$diabetes))
sprintf("Train error GAM: %s", gam_train_error_2)

ypredict_test <- predict(model_gam, newdata = subset(test_data, select = -diabetes), type = "response")
ypredict_test <- as.factor(as.integer(ypredict_test >= 0.5))
gam_test_error_2 = mean((ypredict_test != test_data$diabetes))
sprintf("Test error GAM: %s", gam_test_error_2)
```


```{r}
set.seed(2)

# Grow the tree
classification_tree <- rpart(diabetes ~ . , data = train_data, control = rpart.control(cp = 0), 
                             method = "class")
plotcp(classification_tree)

```

```{r}
classification_tree$cptable


class_tree_10 <- prune(classification_tree, cp = 0.015873016)
par(mar = c(2, 2, 2, 2))
plot(class_tree_10, main = "Pruned Tree")
text(class_tree_10, cex = 0.5, col = "blue")
```

```{r}
tree_train_error_2 <- mean(predict(class_tree_10, subset(train_data, select = -diabetes), type = "class")
                        != train_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_train_error_2)

tree_test_error_2 <- mean(predict(class_tree_10, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Classification Tree: %s", tree_test_error_2)
```


```{r}
bagged_tree <- ipred::bagging(diabetes ~ . , data = train_data, nbagg = 100)
bagged_train_error_2 <- mean(predict(bagged_tree, subset(train_data, select = -diabetes), type = "class")
                           != train_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_train_error_2)

bagged_test_error_2 <- mean(predict(bagged_tree, subset(test_data, select = -diabetes), type = "class") 
                        != test_data$diabetes)
sprintf("Train error Bagged Trees: %s", bagged_test_error_2)
```

Sistemare: !!!!!!!

```{r}
train_data$diabetes <- as.factor(train_data$diabetes)
test_data$diabetes <- as.factor(test_data$diabetes)

#random_forest <- randomForest(diabetes  ~ . , data = train_data, ntree = 100, mtry=2)
#forest_train_error_2 <- mean(predict(random_forest, subset(train_data, select = -diabetes), type = "response")
                #           != train_data$diabetes)
#sprintf("Train error Random Forest: %s", forest_train_error_2)

forest_test_error_2 <- 1 #mean(predict(random_forest, subset(test_data, select = -diabetes), type = "response") 
   #                     != test_data$diabetes)
sprintf("Train error Random Forest: %s", forest_test_error_2)
```

```{r}

```

```{r}

```

```{r}
methods <- c("kNN with 5-fold cv", "kNN with LOO cv", "kNN", "GAM", "Classification Tree",
            "Bagging", "Random Forest")

test_errors <- c(test_err_5fold, test_err_loo, test_err_knn, gam_test_error, tree_test_error,
                 bagged_test_error, forest_test_error)
test_errors_2 <- c(test_err_5fold_2, test_err_loo_2, test_err_knn_2, gam_test_error_2, tree_test_error_2,
                 bagged_test_error_2, forest_test_error_2)
df_comparison <- data.frame(methods, test_errors, test_errors_2)
names(df_comparison) <- c("Method", "Test Error 1", "Test error 2")
df_comparison
```


```{r}

```


```{r}

```


```{r}

```

